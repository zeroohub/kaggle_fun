{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facial keypoints detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as opath\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import errno\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from utils import get_batches, get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting \n",
    "DATA_PATH = 'data/'\n",
    "MODEL_PATH = 'models/'\n",
    "%mkdir -p $MODEL_PATH\n",
    "\n",
    "TRAIN_PATH = opath.join(DATA_PATH, 'train')\n",
    "TEST_PATH = opath.join(DATA_PATH, 'test')\n",
    "VALID_PATH = opath.join(DATA_PATH, 'valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%mkdir -p $DATA_PATH\n",
    "%pwd\n",
    "%cd $DATA_PATH\n",
    "%pwd\n",
    "!kg download -c facial-keypoints-detector\n",
    "%ls\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_pixels(pix_str):\n",
    "    return np.array([int(p) for p in pix_str.split(' ')], 'uint8').reshape((48, 48))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(opath.join(DATA_PATH, 'train.csv'), converters={'Pixels': convert_pixels})\n",
    "    return df\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode_label(df):\n",
    "    df = df.copy()\n",
    "    df.loc[df['Emotion'] == 0, 'Emotion'] = 'anger'\n",
    "    df.loc[df['Emotion'] == 1, 'Emotion'] = 'disgust'\n",
    "    df.loc[df['Emotion'] == 2, 'Emotion'] = 'fear'\n",
    "    df.loc[df['Emotion'] == 3, 'Emotion'] = 'happy'\n",
    "    df.loc[df['Emotion'] == 4, 'Emotion'] = 'sad'\n",
    "    df.loc[df['Emotion'] == 5, 'Emotion'] = 'surprise'\n",
    "    df.loc[df['Emotion'] == 6, 'Emotion'] = 'neutral'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoded_data = decode_label(data)\n",
    "remove_neutral_data = decoded_data[decoded_data['Emotion'] == 'neutral'].sample(700)\n",
    "remove_happy_data = decoded_data[decoded_data['Emotion'] == 'happy'].sample(300)\n",
    "transformed_data = decoded_data.drop((remove_happy_data+remove_neutral_data).index)\n",
    "test_data = transformed_data.sample(frac=0.1)\n",
    "transformed_data = transformed_data.drop(test_data.index)\n",
    "valid_data = transformed_data.sample(frac=0.2)\n",
    "train_data = transformed_data.drop(valid_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "def save_img_files(df, base):\n",
    "    sub_path = opath.join(DATA_PATH, base)\n",
    "    for idx, row in df.iterrows():\n",
    "        category_path = opath.join(sub_path, row['Emotion'])\n",
    "        mkdir(category_path)\n",
    "        Image.fromarray(row['Pixels']).save(opath.join(category_path, '{}.png'.format(idx)), 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_img_files(test_data, 'test')\n",
    "save_img_files(valid_data, 'valid')\n",
    "save_img_files(train_data, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 48, 48)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 24, 24)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 12, 12)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 16135     \n",
      "=================================================================\n",
      "Total params: 39,719\n",
      "Trainable params: 39,719\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_bench_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, activation='relu', padding='same', input_shape=(3, 48, 48)))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, 3, activation='relu', padding='same',))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, 3, activation='relu', padding='same',))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "bench_model = get_bench_model()\n",
    "bench_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=90)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.9472 - acc: 0.1451Epoch 00001: val_loss improved from inf to 1.93008, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 4s 54ms/step - loss: 1.9463 - acc: 0.1465 - val_loss: 1.9301 - val_acc: 0.2010\n",
      "Epoch 2/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.9174 - acc: 0.2129Epoch 00002: val_loss did not improve\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.9117 - acc: 0.2141 - val_loss: 1.9615 - val_acc: 0.1748\n",
      "Epoch 3/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.8279 - acc: 0.2710Epoch 00003: val_loss improved from 1.93008 to 1.76516, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.8338 - acc: 0.2690 - val_loss: 1.7652 - val_acc: 0.2885\n",
      "Epoch 4/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7500 - acc: 0.3165Epoch 00004: val_loss improved from 1.76516 to 1.70012, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.7470 - acc: 0.3163 - val_loss: 1.7001 - val_acc: 0.3322\n",
      "Epoch 5/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6849 - acc: 0.3473Epoch 00005: val_loss improved from 1.70012 to 1.64035, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.6830 - acc: 0.3488 - val_loss: 1.6403 - val_acc: 0.3636\n",
      "Epoch 6/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.6266 - acc: 0.3682Epoch 00006: val_loss improved from 1.64035 to 1.56474, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.6235 - acc: 0.3711 - val_loss: 1.5647 - val_acc: 0.4126\n",
      "Epoch 7/30\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 1.5834 - acc: 0.3883Epoch 00007: val_loss improved from 1.56474 to 1.52596, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.5877 - acc: 0.3860 - val_loss: 1.5260 - val_acc: 0.4790\n",
      "Epoch 8/30\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 1.5433 - acc: 0.4301Epoch 00008: val_loss improved from 1.52596 to 1.47238, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.5517 - acc: 0.4232 - val_loss: 1.4724 - val_acc: 0.4493\n",
      "Epoch 9/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4925 - acc: 0.4313Epoch 00009: val_loss did not improve\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.4958 - acc: 0.4274 - val_loss: 1.4827 - val_acc: 0.4528\n",
      "Epoch 10/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4700 - acc: 0.4366Epoch 00010: val_loss improved from 1.47238 to 1.40601, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.4708 - acc: 0.4333 - val_loss: 1.4060 - val_acc: 0.4773\n",
      "Epoch 11/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.4462 - acc: 0.4438Epoch 00011: val_loss improved from 1.40601 to 1.38441, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.4488 - acc: 0.4459 - val_loss: 1.3844 - val_acc: 0.4930\n",
      "Epoch 12/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4399 - acc: 0.4522Epoch 00012: val_loss improved from 1.38441 to 1.35111, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.4369 - acc: 0.4573 - val_loss: 1.3511 - val_acc: 0.5017\n",
      "Epoch 13/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4057 - acc: 0.4768Epoch 00013: val_loss improved from 1.35111 to 1.28091, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.4035 - acc: 0.4755 - val_loss: 1.2809 - val_acc: 0.5280\n",
      "Epoch 14/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3802 - acc: 0.4813Epoch 00014: val_loss did not improve\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.3781 - acc: 0.4831 - val_loss: 1.2936 - val_acc: 0.5472\n",
      "Epoch 15/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3668 - acc: 0.4906Epoch 00015: val_loss improved from 1.28091 to 1.26485, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.3743 - acc: 0.4894 - val_loss: 1.2648 - val_acc: 0.5717\n",
      "Epoch 16/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3610 - acc: 0.4960Epoch 00016: val_loss improved from 1.26485 to 1.24160, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.3685 - acc: 0.4954 - val_loss: 1.2416 - val_acc: 0.5612\n",
      "Epoch 17/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.3255 - acc: 0.4932Epoch 00017: val_loss improved from 1.24160 to 1.22376, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.3244 - acc: 0.4953 - val_loss: 1.2238 - val_acc: 0.5472\n",
      "Epoch 18/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.3192 - acc: 0.5041Epoch 00018: val_loss improved from 1.22376 to 1.21914, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.3150 - acc: 0.5034 - val_loss: 1.2191 - val_acc: 0.5490\n",
      "Epoch 19/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3048 - acc: 0.5232Epoch 00019: val_loss improved from 1.21914 to 1.18073, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.3008 - acc: 0.5220 - val_loss: 1.1807 - val_acc: 0.5804\n",
      "Epoch 20/30\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 1.2953 - acc: 0.5257Epoch 00020: val_loss did not improve\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.2733 - acc: 0.5346 - val_loss: 1.2328 - val_acc: 0.5699\n",
      "Epoch 21/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2636 - acc: 0.5214Epoch 00021: val_loss improved from 1.18073 to 1.15676, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.2662 - acc: 0.5244 - val_loss: 1.1568 - val_acc: 0.5944\n",
      "Epoch 22/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2796 - acc: 0.5322- ETA: 0s - loEpoch 00022: val_loss did not improve\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.2756 - acc: 0.5388 - val_loss: 1.1588 - val_acc: 0.5874\n",
      "Epoch 23/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2461 - acc: 0.5362Epoch 00023: val_loss improved from 1.15676 to 1.12636, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.2441 - acc: 0.5342 - val_loss: 1.1264 - val_acc: 0.5892\n",
      "Epoch 24/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2543 - acc: 0.5231Epoch 00024: val_loss improved from 1.12636 to 1.09183, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 3s 36ms/step - loss: 1.2440 - acc: 0.5304 - val_loss: 1.0918 - val_acc: 0.5944\n",
      "Epoch 25/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2283 - acc: 0.5263Epoch 00025: val_loss did not improve\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.2369 - acc: 0.5228 - val_loss: 1.0974 - val_acc: 0.6101\n",
      "Epoch 26/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2117 - acc: 0.5494Epoch 00026: val_loss improved from 1.09183 to 1.07674, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.2175 - acc: 0.5548 - val_loss: 1.0767 - val_acc: 0.6154\n",
      "Epoch 27/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2145 - acc: 0.5476Epoch 00027: val_loss improved from 1.07674 to 1.07642, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 34ms/step - loss: 1.2096 - acc: 0.5506 - val_loss: 1.0764 - val_acc: 0.6066\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/71 [============================>.] - ETA: 0s - loss: 1.1917 - acc: 0.5598Epoch 00028: val_loss improved from 1.07642 to 1.05105, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.1946 - acc: 0.5574 - val_loss: 1.0510 - val_acc: 0.6189\n",
      "Epoch 29/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.1802 - acc: 0.5611Epoch 00029: val_loss improved from 1.05105 to 1.03693, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.1709 - acc: 0.5654 - val_loss: 1.0369 - val_acc: 0.6241\n",
      "Epoch 30/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.1526 - acc: 0.5741Epoch 00030: val_loss did not improve\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.1570 - acc: 0.5727 - val_loss: 1.0719 - val_acc: 0.6101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f120440d490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "bench_weights_path = opath.join(MODEL_PATH, 'bench.weights.best.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=bench_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen, (48, 48))\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen, (48, 48))\n",
    "\n",
    "bench_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//batch_size, \n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "benchmark model get accuary 0.522012578991%\n"
     ]
    }
   ],
   "source": [
    "bench_model.load_weights(bench_weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (48, 48))\n",
    "\n",
    "print \"benchmark model get accuary {}%\".format(get_accuracy(bench_model, test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_4 (Lambda)            (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_40 (ZeroPaddi (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPaddi (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPaddi (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPaddi (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPaddi (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPaddi (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPaddi (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPaddi (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_49 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_50 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_51 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_52 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 7)                 28679     \n",
      "=================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 28,679\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.vgg16 import get_model\n",
    "vgg_model = get_model()\n",
    "\n",
    "vgg_model.pop()\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "valid_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 2.7245 - acc: 0.2357Epoch 00001: val_loss improved from inf to 1.80391, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 13s 179ms/step - loss: 2.7046 - acc: 0.2348 - val_loss: 1.8039 - val_acc: 0.2640\n",
      "Epoch 2/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 2.2560 - acc: 0.3138- ETA: 3Epoch 00002: val_loss improved from 1.80391 to 1.48283, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 2.2808 - acc: 0.3088 - val_loss: 1.4828 - val_acc: 0.4650\n",
      "Epoch 3/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 2.0237 - acc: 0.3799- ETA: 0s - loss: 2.0459 - acc:Epoch 00003: val_loss improved from 1.48283 to 1.34133, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 2.0207 - acc: 0.3813 - val_loss: 1.3413 - val_acc: 0.5367\n",
      "Epoch 4/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.9136 - acc: 0.4009Epoch 00004: val_loss improved from 1.34133 to 1.18153, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 1.9427 - acc: 0.3953 - val_loss: 1.1815 - val_acc: 0.5542\n",
      "Epoch 5/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.8332 - acc: 0.4228Epoch 00005: val_loss did not improve\n",
      "72/71 [==============================] - 9s 128ms/step - loss: 1.8136 - acc: 0.4310 - val_loss: 1.5151 - val_acc: 0.4318\n",
      "Epoch 6/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.8729 - acc: 0.4161Epoch 00006: val_loss improved from 1.18153 to 1.13534, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 137ms/step - loss: 1.8700 - acc: 0.4147 - val_loss: 1.1353 - val_acc: 0.5944\n",
      "Epoch 7/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7023 - acc: 0.4701Epoch 00007: val_loss did not improve\n",
      "72/71 [==============================] - 11s 156ms/step - loss: 1.7033 - acc: 0.4717 - val_loss: 1.2785 - val_acc: 0.5507\n",
      "Epoch 8/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7153 - acc: 0.4616Epoch 00008: val_loss improved from 1.13534 to 1.05212, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 144ms/step - loss: 1.7100 - acc: 0.4645 - val_loss: 1.0521 - val_acc: 0.6276\n",
      "Epoch 9/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6795 - acc: 0.4763Epoch 00009: val_loss did not improve\n",
      "72/71 [==============================] - 9s 129ms/step - loss: 1.6971 - acc: 0.4751 - val_loss: 1.0996 - val_acc: 0.6206\n",
      "Epoch 10/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6565 - acc: 0.4598- ETA: 0s - loss: 1.6632 - acc: Epoch 00010: val_loss did not improve\n",
      "72/71 [==============================] - 9s 130ms/step - loss: 1.6474 - acc: 0.4628 - val_loss: 1.2665 - val_acc: 0.5420\n",
      "Epoch 11/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6051 - acc: 0.4786- ETA: 0s - loss: 1.6009 - acc: 0.479Epoch 00011: val_loss did not improve\n",
      "72/71 [==============================] - 10s 132ms/step - loss: 1.6183 - acc: 0.4764 - val_loss: 1.1482 - val_acc: 0.5752\n",
      "Epoch 12/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6159 - acc: 0.4763- ETA: 0s - loss: 1.6345 - acc: Epoch 00012: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.6205 - acc: 0.4734 - val_loss: 1.2582 - val_acc: 0.5612\n",
      "Epoch 13/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5659 - acc: 0.4893- ETA: 0s - loss: 1.5601 - acc: 0Epoch 00013: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.5497 - acc: 0.4940 - val_loss: 1.0672 - val_acc: 0.5997\n",
      "Epoch 14/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5660 - acc: 0.4978Epoch 00014: val_loss did not improve\n",
      "72/71 [==============================] - 10s 134ms/step - loss: 1.5620 - acc: 0.4979 - val_loss: 1.1346 - val_acc: 0.5839\n",
      "Epoch 15/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6011 - acc: 0.4973Epoch 00015: val_loss did not improve\n",
      "72/71 [==============================] - 9s 130ms/step - loss: 1.6079 - acc: 0.4983 - val_loss: 1.2147 - val_acc: 0.5944\n",
      "Epoch 16/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5814 - acc: 0.5089Epoch 00016: val_loss did not improve\n",
      "72/71 [==============================] - 10s 133ms/step - loss: 1.5789 - acc: 0.5076 - val_loss: 1.2084 - val_acc: 0.5892\n",
      "Epoch 17/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5589 - acc: 0.5071Epoch 00017: val_loss did not improve\n",
      "72/71 [==============================] - 10s 132ms/step - loss: 1.5620 - acc: 0.5051 - val_loss: 1.2358 - val_acc: 0.5717\n",
      "Epoch 18/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5638 - acc: 0.5013Epoch 00018: val_loss did not improve\n",
      "72/71 [==============================] - 10s 132ms/step - loss: 1.5504 - acc: 0.5046 - val_loss: 1.3301 - val_acc: 0.5350\n",
      "Epoch 19/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5122 - acc: 0.5170Epoch 00019: val_loss did not improve\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 1.5107 - acc: 0.5186 - val_loss: 1.1205 - val_acc: 0.5997\n",
      "Epoch 20/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5553 - acc: 0.5058Epoch 00020: val_loss did not improve\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 1.5575 - acc: 0.5046 - val_loss: 1.3743 - val_acc: 0.5367\n",
      "Epoch 21/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6063 - acc: 0.4942Epoch 00021: val_loss did not improve\n",
      "72/71 [==============================] - 10s 137ms/step - loss: 1.6085 - acc: 0.4945 - val_loss: 1.0523 - val_acc: 0.6136\n",
      "Epoch 22/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5394 - acc: 0.5125Epoch 00022: val_loss did not improve\n",
      "72/71 [==============================] - 10s 132ms/step - loss: 1.5506 - acc: 0.5127 - val_loss: 1.1148 - val_acc: 0.5909\n",
      "Epoch 23/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4843 - acc: 0.5268- ETA: 0s - loss: 1.4865 - acc: 0.526Epoch 00023: val_loss did not improve\n",
      "72/71 [==============================] - 10s 135ms/step - loss: 1.4831 - acc: 0.5312 - val_loss: 1.2716 - val_acc: 0.5385\n",
      "Epoch 24/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5365 - acc: 0.5161Epoch 00024: val_loss did not improve\n",
      "72/71 [==============================] - 10s 134ms/step - loss: 1.5434 - acc: 0.5135 - val_loss: 1.3044 - val_acc: 0.5542\n",
      "Epoch 25/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5646 - acc: 0.5098Epoch 00025: val_loss did not improve\n",
      "72/71 [==============================] - 10s 132ms/step - loss: 1.5746 - acc: 0.5068 - val_loss: 1.2183 - val_acc: 0.5857\n",
      "Epoch 26/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5507 - acc: 0.5201Epoch 00026: val_loss improved from 1.05212 to 1.03964, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 140ms/step - loss: 1.5398 - acc: 0.5223 - val_loss: 1.0396 - val_acc: 0.6381\n",
      "Epoch 27/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5646 - acc: 0.4946Epoch 00027: val_loss did not improve\n",
      "72/71 [==============================] - 9s 130ms/step - loss: 1.5910 - acc: 0.4874 - val_loss: 1.1981 - val_acc: 0.5769\n",
      "Epoch 28/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5157 - acc: 0.5241Epoch 00028: val_loss did not improve\n",
      "72/71 [==============================] - 9s 130ms/step - loss: 1.5172 - acc: 0.5253 - val_loss: 1.1028 - val_acc: 0.6049\n",
      "Epoch 29/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5553 - acc: 0.5125Epoch 00029: val_loss did not improve\n",
      "72/71 [==============================] - 11s 152ms/step - loss: 1.5189 - acc: 0.5218 - val_loss: 1.0545 - val_acc: 0.6084\n",
      "Epoch 30/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5130 - acc: 0.5241Epoch 00030: val_loss did not improve\n",
      "72/71 [==============================] - 14s 189ms/step - loss: 1.5076 - acc: 0.5245 - val_loss: 1.2907 - val_acc: 0.5664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f358049dfd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_weights_path = opath.join(MODEL_PATH, 'vgg_tune.weights.best.hdf5')\n",
    "checkpointer = ModelCheckpoint(filepath=top_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen)\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen)\n",
    "\n",
    "vgg_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//train_batches.batch_size,\n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//valid_batches.batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=30, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "vgg16 model finetune top layer get accuary 0.635220125411%\n"
     ]
    }
   ],
   "source": [
    "vgg_model.load_weights(top_weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (224, 224))\n",
    "\n",
    "print \"vgg16 model finetune top layer get accuary {}%\".format(get_accuracy(vgg_model, test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vgg16 fintune all dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lambda_4\n",
      "1 zero_padding2d_40\n",
      "2 conv2d_40\n",
      "3 zero_padding2d_41\n",
      "4 conv2d_41\n",
      "5 max_pooling2d_16\n",
      "6 zero_padding2d_42\n",
      "7 conv2d_42\n",
      "8 zero_padding2d_43\n",
      "9 conv2d_43\n",
      "10 max_pooling2d_17\n",
      "11 zero_padding2d_44\n",
      "12 conv2d_44\n",
      "13 zero_padding2d_45\n",
      "14 conv2d_45\n",
      "15 zero_padding2d_46\n",
      "16 conv2d_46\n",
      "17 max_pooling2d_18\n",
      "18 zero_padding2d_47\n",
      "19 conv2d_47\n",
      "20 zero_padding2d_48\n",
      "21 conv2d_48\n",
      "22 zero_padding2d_49\n",
      "23 conv2d_49\n",
      "24 max_pooling2d_19\n",
      "25 zero_padding2d_50\n",
      "26 conv2d_50\n",
      "27 zero_padding2d_51\n",
      "28 conv2d_51\n",
      "29 zero_padding2d_52\n",
      "30 conv2d_52\n",
      "31 max_pooling2d_20\n",
      "32 flatten_4\n",
      "33 dense_12\n",
      "34 dropout_7\n",
      "35 dense_13\n",
      "36 dropout_8\n",
      "37 dense_15\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(vgg_model.layers):\n",
    "    print idx, layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg_model.layers[:33]:\n",
    "    layer.trainable = False\n",
    "for layer in vgg_model.layers[33:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(0.001, 0.9, 0.0001, True)\n",
    "vgg_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.8598 - acc: 0.4156Epoch 00001: val_loss improved from inf to 0.93653, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 14s 199ms/step - loss: 1.8245 - acc: 0.4218 - val_loss: 0.9365 - val_acc: 0.6521\n",
      "Epoch 2/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.1411 - acc: 0.6013Epoch 00002: val_loss did not improve\n",
      "72/71 [==============================] - 11s 157ms/step - loss: 1.1461 - acc: 0.6001 - val_loss: 1.0541 - val_acc: 0.5839\n",
      "Epoch 3/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.8853 - acc: 0.6714Epoch 00003: val_loss improved from 0.93653 to 0.85498, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 12s 168ms/step - loss: 0.8879 - acc: 0.6739 - val_loss: 0.8550 - val_acc: 0.6818\n",
      "Epoch 4/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.7843 - acc: 0.7219Epoch 00004: val_loss improved from 0.85498 to 0.71439, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 16s 227ms/step - loss: 0.7730 - acc: 0.7234 - val_loss: 0.7144 - val_acc: 0.7483\n",
      "Epoch 5/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.7741- ETA: 3s - Epoch 00005: val_loss improved from 0.71439 to 0.69832, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 15s 210ms/step - loss: 0.6304 - acc: 0.7753 - val_loss: 0.6983 - val_acc: 0.7657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f359023b9d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_weights_path = opath.join(MODEL_PATH, 'vgg_tune_dense.weights.best.hdf5')\n",
    "checkpointer = ModelCheckpoint(filepath=dense_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen)\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen)\n",
    "\n",
    "vgg_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//train_batches.batch_size,\n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//valid_batches.batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "vgg16 model finetune dense layer get accuary 0.738993709942%\n"
     ]
    }
   ],
   "source": [
    "vgg_model.load_weights(weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (224, 224))\n",
    "\n",
    "print \"vgg16 model finetune dense layer get accuary {}%\".format(get_accuracy(vgg_model, test_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
