{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facial keypoints detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as opath\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import errno\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# add utils into PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from utils import get_batches, get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ENV settings\n",
    "DATA_PATH = 'data/'\n",
    "MODEL_PATH = 'models/'\n",
    "%mkdir -p $MODEL_PATH\n",
    "\n",
    "TRAIN_PATH = opath.join(DATA_PATH, 'train')\n",
    "TEST_PATH = opath.join(DATA_PATH, 'test')\n",
    "VALID_PATH = opath.join(DATA_PATH, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ansible/workspace/kaggle_fun/facial_keypoints_detector/data\n",
      "downloading https://www.kaggle.com/c/facial-keypoints-detector/download/train.csv\n",
      "\n",
      "train.csv 100% |#####################################| Time: 0:00:01  20.0 MiB/s\n",
      "\n",
      "downloading https://www.kaggle.com/c/facial-keypoints-detector/download/test.csv\n",
      "\n",
      "test.csv 100% |######################################| Time: 0:00:00  13.6 MiB/s\n",
      "\n",
      "downloading https://www.kaggle.com/c/facial-keypoints-detector/download/train_identity.csv\n",
      "\n",
      "train_identity.csv 100% |############################| Time: 0:00:00  62.8 KiB/s\n",
      "\n",
      "test.csv  train.csv  train_identity.csv\n",
      "/home/ansible/workspace/kaggle_fun/facial_keypoints_detector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'/home/ansible/workspace/kaggle_fun/facial_keypoints_detector'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mkdir -p $DATA_PATH\n",
    "%pwd\n",
    "%cd $DATA_PATH\n",
    "%pwd\n",
    "!kg download -c facial-keypoints-detector\n",
    "%ls\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_pixels(pix_str):\n",
    "    return np.array([int(p) for p in pix_str.split(' ')], 'uint8').reshape((48, 48))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(opath.join(DATA_PATH, 'train.csv'), converters={'Pixels': convert_pixels})\n",
    "    return df\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode_label(df):\n",
    "    df = df.copy()\n",
    "    df.loc[df['Emotion'] == 0, 'Emotion'] = 'anger'\n",
    "    df.loc[df['Emotion'] == 1, 'Emotion'] = 'disgust'\n",
    "    df.loc[df['Emotion'] == 2, 'Emotion'] = 'fear'\n",
    "    df.loc[df['Emotion'] == 3, 'Emotion'] = 'happy'\n",
    "    df.loc[df['Emotion'] == 4, 'Emotion'] = 'sad'\n",
    "    df.loc[df['Emotion'] == 5, 'Emotion'] = 'surprise'\n",
    "    df.loc[df['Emotion'] == 6, 'Emotion'] = 'neutral'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoded_data = decode_label(data)\n",
    "remove_neutral_data = decoded_data[decoded_data['Emotion'] == 'neutral'].sample(700)\n",
    "remove_happy_data = decoded_data[decoded_data['Emotion'] == 'happy'].sample(300)\n",
    "transformed_data = decoded_data.drop((remove_happy_data+remove_neutral_data).index)\n",
    "test_data = transformed_data.sample(frac=0.1)\n",
    "transformed_data = transformed_data.drop(test_data.index)\n",
    "valid_data = transformed_data.sample(frac=0.2)\n",
    "train_data = transformed_data.drop(valid_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "def save_img_files(df, base):\n",
    "    sub_path = opath.join(DATA_PATH, base)\n",
    "    for idx, row in df.iterrows():\n",
    "        category_path = opath.join(sub_path, row['Emotion'])\n",
    "        mkdir(category_path)\n",
    "        Image.fromarray(row['Pixels']).save(opath.join(category_path, '{}.png'.format(idx)), 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_img_files(test_data, 'test')\n",
    "save_img_files(valid_data, 'valid')\n",
    "save_img_files(train_data, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 48, 48)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 24, 24)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 12, 12)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 16135     \n",
      "=================================================================\n",
      "Total params: 39,719\n",
      "Trainable params: 39,719\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_bench_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, activation='relu', padding='same', input_shape=(3, 48, 48)))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, 3, activation='relu', padding='same',))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, 3, activation='relu', padding='same',))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "bench_model = get_bench_model()\n",
    "bench_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=90)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.9496 - acc: 0.1437Epoch 00001: val_loss improved from inf to 1.93761, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 4s 59ms/step - loss: 1.9490 - acc: 0.1444 - val_loss: 1.9376 - val_acc: 0.1591\n",
      "Epoch 2/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.9294 - acc: 0.1943Epoch 00002: val_loss improved from 1.93761 to 1.89195, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.9286 - acc: 0.1984 - val_loss: 1.8920 - val_acc: 0.2797\n",
      "Epoch 3/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.8774 - acc: 0.2495Epoch 00003: val_loss improved from 1.89195 to 1.81153, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 34ms/step - loss: 1.8713 - acc: 0.2571 - val_loss: 1.8115 - val_acc: 0.2762\n",
      "Epoch 4/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.8080 - acc: 0.2808Epoch 00004: val_loss improved from 1.81153 to 1.73344, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 28ms/step - loss: 1.8077 - acc: 0.2796 - val_loss: 1.7334 - val_acc: 0.3287\n",
      "Epoch 5/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7374 - acc: 0.3281Epoch 00005: val_loss improved from 1.73344 to 1.71863, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.7381 - acc: 0.3248 - val_loss: 1.7186 - val_acc: 0.3409\n",
      "Epoch 6/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.6964 - acc: 0.3356Epoch 00006: val_loss improved from 1.71863 to 1.64296, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.7017 - acc: 0.3337 - val_loss: 1.6430 - val_acc: 0.3706\n",
      "Epoch 7/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.6386 - acc: 0.3755Epoch 00007: val_loss improved from 1.64296 to 1.63358, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.6262 - acc: 0.3792 - val_loss: 1.6336 - val_acc: 0.3864\n",
      "Epoch 8/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.6033 - acc: 0.3958Epoch 00008: val_loss improved from 1.63358 to 1.49853, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.6005 - acc: 0.3961 - val_loss: 1.4985 - val_acc: 0.4406\n",
      "Epoch 9/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5477 - acc: 0.4268Epoch 00009: val_loss improved from 1.49853 to 1.48474, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.5449 - acc: 0.4315 - val_loss: 1.4847 - val_acc: 0.4388\n",
      "Epoch 10/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.5112 - acc: 0.4407Epoch 00010: val_loss improved from 1.48474 to 1.42877, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.5073 - acc: 0.4376 - val_loss: 1.4288 - val_acc: 0.4668\n",
      "Epoch 11/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.4831 - acc: 0.4348Epoch 00011: val_loss improved from 1.42877 to 1.40362, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.4851 - acc: 0.4362 - val_loss: 1.4036 - val_acc: 0.4790\n",
      "Epoch 12/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.4773 - acc: 0.4488Epoch 00012: val_loss improved from 1.40362 to 1.35392, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.4708 - acc: 0.4493 - val_loss: 1.3539 - val_acc: 0.5122\n",
      "Epoch 13/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.4358 - acc: 0.4692Epoch 00013: val_loss improved from 1.35392 to 1.33526, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.4392 - acc: 0.4654 - val_loss: 1.3353 - val_acc: 0.5070\n",
      "Epoch 14/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4290 - acc: 0.4612Epoch 00014: val_loss improved from 1.33526 to 1.29189, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.4302 - acc: 0.4624 - val_loss: 1.2919 - val_acc: 0.5035\n",
      "Epoch 15/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.4029 - acc: 0.4787Epoch 00015: val_loss improved from 1.29189 to 1.25032, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.4017 - acc: 0.4789 - val_loss: 1.2503 - val_acc: 0.5367\n",
      "Epoch 16/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.3561 - acc: 0.5000Epoch 00016: val_loss improved from 1.25032 to 1.24477, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.3587 - acc: 0.4975 - val_loss: 1.2448 - val_acc: 0.5280\n",
      "Epoch 17/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.3729 - acc: 0.4941Epoch 00017: val_loss improved from 1.24477 to 1.18249, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.3671 - acc: 0.4928 - val_loss: 1.1825 - val_acc: 0.5542\n",
      "Epoch 18/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.3499 - acc: 0.5014Epoch 00018: val_loss did not improve\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.3477 - acc: 0.5013 - val_loss: 1.2077 - val_acc: 0.5664\n",
      "Epoch 19/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3167 - acc: 0.5013Epoch 00019: val_loss did not improve\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.3115 - acc: 0.5071 - val_loss: 1.2181 - val_acc: 0.5490\n",
      "Epoch 20/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3247 - acc: 0.5045Epoch 00020: val_loss did not improve\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.3228 - acc: 0.5051 - val_loss: 1.1836 - val_acc: 0.5542\n",
      "Epoch 21/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.3428 - acc: 0.4986Epoch 00021: val_loss did not improve\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.3404 - acc: 0.5008 - val_loss: 1.1854 - val_acc: 0.5769\n",
      "Epoch 22/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2923 - acc: 0.5339Epoch 00022: val_loss did not improve\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.2907 - acc: 0.5354 - val_loss: 1.1934 - val_acc: 0.5752\n",
      "Epoch 23/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2590 - acc: 0.5304Epoch 00023: val_loss improved from 1.18249 to 1.15747, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.2637 - acc: 0.5270 - val_loss: 1.1575 - val_acc: 0.5734\n",
      "Epoch 24/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2915 - acc: 0.5154Epoch 00024: val_loss improved from 1.15747 to 1.13758, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 29ms/step - loss: 1.2908 - acc: 0.5181 - val_loss: 1.1376 - val_acc: 0.5524\n",
      "Epoch 25/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2546 - acc: 0.5254Epoch 00025: val_loss did not improve\n",
      "72/71 [==============================] - 2s 30ms/step - loss: 1.2486 - acc: 0.5304 - val_loss: 1.1382 - val_acc: 0.5717\n",
      "Epoch 26/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2572 - acc: 0.5349Epoch 00026: val_loss improved from 1.13758 to 1.11531, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 31ms/step - loss: 1.2472 - acc: 0.5393 - val_loss: 1.1153 - val_acc: 0.5927\n",
      "Epoch 27/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2281 - acc: 0.5446Epoch 00027: val_loss did not improve\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.2163 - acc: 0.5522 - val_loss: 1.1172 - val_acc: 0.5769\n",
      "Epoch 28/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2353 - acc: 0.5476Epoch 00028: val_loss improved from 1.11531 to 1.05387, saving model to models/bench.weights.best.hdf5\n",
      "72/71 [==============================] - 2s 32ms/step - loss: 1.2377 - acc: 0.5485 - val_loss: 1.0539 - val_acc: 0.6084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "69/71 [============================>.] - ETA: 0s - loss: 1.2004 - acc: 0.5575Epoch 00029: val_loss did not improve\n",
      "72/71 [==============================] - 2s 34ms/step - loss: 1.2030 - acc: 0.5520 - val_loss: 1.0797 - val_acc: 0.5839\n",
      "Epoch 30/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2312 - acc: 0.5442Epoch 00030: val_loss did not improve\n",
      "72/71 [==============================] - 2s 33ms/step - loss: 1.2282 - acc: 0.5477 - val_loss: 1.0767 - val_acc: 0.5804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1068b5bbd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "bench_weights_path = opath.join(MODEL_PATH, 'bench.weights.best.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=bench_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen, (48, 48))\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen, (48, 48))\n",
    "\n",
    "bench_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//batch_size, \n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "benchmark model get accuary 0.487421383835%\n"
     ]
    }
   ],
   "source": [
    "bench_model.load_weights(bench_weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (48, 48))\n",
    "\n",
    "print \"benchmark model get accuary {}%\".format(get_accuracy(bench_model, test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 28679     \n",
      "=================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 28,679\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.vgg16 import get_model\n",
    "vgg_model = get_model()\n",
    "\n",
    "vgg_model.pop()\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "valid_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 2.7078 - acc: 0.2424Epoch 00001: val_loss improved from inf to 1.89160, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 12s 163ms/step - loss: 2.7104 - acc: 0.2411 - val_loss: 1.8916 - val_acc: 0.3619\n",
      "Epoch 2/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 2.1868 - acc: 0.3388Epoch 00002: val_loss improved from 1.89160 to 1.43074, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 135ms/step - loss: 2.1936 - acc: 0.3408 - val_loss: 1.4307 - val_acc: 0.4423\n",
      "Epoch 3/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.9889 - acc: 0.3911Epoch 00003: val_loss improved from 1.43074 to 1.40914, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 1.9723 - acc: 0.3869 - val_loss: 1.4091 - val_acc: 0.5105\n",
      "Epoch 4/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.8860 - acc: 0.4210Epoch 00004: val_loss improved from 1.40914 to 1.28778, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 136ms/step - loss: 1.8889 - acc: 0.4202 - val_loss: 1.2878 - val_acc: 0.5227\n",
      "Epoch 5/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.8438 - acc: 0.4397Epoch 00005: val_loss did not improve\n",
      "72/71 [==============================] - 9s 128ms/step - loss: 1.8453 - acc: 0.4371 - val_loss: 1.3000 - val_acc: 0.5052\n",
      "Epoch 6/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7513 - acc: 0.4388Epoch 00006: val_loss did not improve\n",
      "72/71 [==============================] - 9s 128ms/step - loss: 1.7558 - acc: 0.4396 - val_loss: 1.2943 - val_acc: 0.5035\n",
      "Epoch 7/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6928 - acc: 0.4799- ETA: 0s - loss: 1.6853 - acc: 0.Epoch 00007: val_loss did not improve\n",
      "72/71 [==============================] - 9s 128ms/step - loss: 1.6872 - acc: 0.4768 - val_loss: 1.4373 - val_acc: 0.4930\n",
      "Epoch 8/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6800 - acc: 0.4750- ETA: 0s - loss: 1.6900 - acc: 0Epoch 00008: val_loss improved from 1.28778 to 1.10137, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 138ms/step - loss: 1.6866 - acc: 0.4697 - val_loss: 1.1014 - val_acc: 0.5962\n",
      "Epoch 9/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7049 - acc: 0.4701Epoch 00009: val_loss did not improve\n",
      "72/71 [==============================] - 9s 129ms/step - loss: 1.6857 - acc: 0.4726 - val_loss: 1.2186 - val_acc: 0.5245\n",
      "Epoch 10/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5873 - acc: 0.4942Epoch 00010: val_loss did not improve\n",
      "72/71 [==============================] - 9s 129ms/step - loss: 1.6016 - acc: 0.4920 - val_loss: 1.4700 - val_acc: 0.5052\n",
      "Epoch 11/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6032 - acc: 0.5103- ETA: 0s - loss: 1.5931 - acc: 0Epoch 00011: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.5976 - acc: 0.5097 - val_loss: 1.3317 - val_acc: 0.5297\n",
      "Epoch 12/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5692 - acc: 0.4955Epoch 00012: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.5824 - acc: 0.4908 - val_loss: 1.2244 - val_acc: 0.5402\n",
      "Epoch 13/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6046 - acc: 0.5027Epoch 00013: val_loss improved from 1.10137 to 1.09538, saving model to models/vgg_tune.weights.best.hdf5\n",
      "72/71 [==============================] - 10s 143ms/step - loss: 1.6256 - acc: 0.4984 - val_loss: 1.0954 - val_acc: 0.5892\n",
      "Epoch 14/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5564 - acc: 0.4996Epoch 00014: val_loss did not improve\n",
      "72/71 [==============================] - 10s 135ms/step - loss: 1.5566 - acc: 0.4988 - val_loss: 1.2412 - val_acc: 0.5245\n",
      "Epoch 15/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5430 - acc: 0.5112Epoch 00015: val_loss did not improve\n",
      "72/71 [==============================] - 10s 134ms/step - loss: 1.5833 - acc: 0.5047 - val_loss: 1.1918 - val_acc: 0.5524\n",
      "Epoch 16/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5421 - acc: 0.5112- ETA: 0s - loss: 1.5393 - acc: 0.511Epoch 00016: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.5405 - acc: 0.5081 - val_loss: 1.2586 - val_acc: 0.5402\n",
      "Epoch 17/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5279 - acc: 0.5085Epoch 00017: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.5142 - acc: 0.5147 - val_loss: 1.3207 - val_acc: 0.5245\n",
      "Epoch 18/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5481 - acc: 0.5165Epoch 00018: val_loss did not improve\n",
      "72/71 [==============================] - 9s 131ms/step - loss: 1.5538 - acc: 0.5164 - val_loss: 1.2526 - val_acc: 0.5490\n",
      "Epoch 19/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5060 - acc: 0.5138Epoch 00019: val_loss did not improve\n",
      "72/71 [==============================] - 10s 135ms/step - loss: 1.5083 - acc: 0.5156 - val_loss: 1.1150 - val_acc: 0.5944\n",
      "Epoch 20/20\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5262 - acc: 0.5321- ETA: 0s - loss: 1.5055 - acc: 0.5Epoch 00020: val_loss did not improve\n",
      "72/71 [==============================] - 10s 137ms/step - loss: 1.5109 - acc: 0.5320 - val_loss: 1.1274 - val_acc: 0.5752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fdcfa58d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_weights_path = opath.join(MODEL_PATH, 'vgg_tune_top.weights.best.hdf5')\n",
    "checkpointer = ModelCheckpoint(filepath=top_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen)\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen)\n",
    "\n",
    "vgg_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//train_batches.batch_size,\n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//valid_batches.batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=20, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "vgg16 model finetune top layer get accuary 0.619496855908%\n"
     ]
    }
   ],
   "source": [
    "vgg_model.load_weights(top_weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (224, 224))\n",
    "\n",
    "print \"vgg16 model finetune top layer get accuary {}%\".format(get_accuracy(vgg_model, test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lambda_4\n",
      "1 zero_padding2d_40\n",
      "2 conv2d_40\n",
      "3 zero_padding2d_41\n",
      "4 conv2d_41\n",
      "5 max_pooling2d_16\n",
      "6 zero_padding2d_42\n",
      "7 conv2d_42\n",
      "8 zero_padding2d_43\n",
      "9 conv2d_43\n",
      "10 max_pooling2d_17\n",
      "11 zero_padding2d_44\n",
      "12 conv2d_44\n",
      "13 zero_padding2d_45\n",
      "14 conv2d_45\n",
      "15 zero_padding2d_46\n",
      "16 conv2d_46\n",
      "17 max_pooling2d_18\n",
      "18 zero_padding2d_47\n",
      "19 conv2d_47\n",
      "20 zero_padding2d_48\n",
      "21 conv2d_48\n",
      "22 zero_padding2d_49\n",
      "23 conv2d_49\n",
      "24 max_pooling2d_19\n",
      "25 zero_padding2d_50\n",
      "26 conv2d_50\n",
      "27 zero_padding2d_51\n",
      "28 conv2d_51\n",
      "29 zero_padding2d_52\n",
      "30 conv2d_52\n",
      "31 max_pooling2d_20\n",
      "32 flatten_4\n",
      "33 dense_12\n",
      "34 dropout_7\n",
      "35 dense_13\n",
      "36 dropout_8\n",
      "37 dense_15\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(vgg_model.layers):\n",
    "    print idx, layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg_model.layers[:33]:\n",
    "    layer.trainable = False\n",
    "for layer in vgg_model.layers[33:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "optmizer = SGD(0.001, 0.9, 0.0001, True)\n",
    "vgg_model.compile(optimizer=optmizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6374 - acc: 0.4589Epoch 00001: val_loss improved from inf to 0.95209, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 95s 1s/step - loss: 1.6240 - acc: 0.4578 - val_loss: 0.9521 - val_acc: 0.6224\n",
      "Epoch 2/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.0056 - acc: 0.6482Epoch 00002: val_loss improved from 0.95209 to 0.88671, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 56s 776ms/step - loss: 1.0083 - acc: 0.6486 - val_loss: 0.8867 - val_acc: 0.6748\n",
      "Epoch 3/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.7902 - acc: 0.7089Epoch 00003: val_loss improved from 0.88671 to 0.72222, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 44s 614ms/step - loss: 0.7933 - acc: 0.7078 - val_loss: 0.7222 - val_acc: 0.7343\n",
      "Epoch 4/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.7554Epoch 00004: val_loss did not improve\n",
      "72/71 [==============================] - 44s 612ms/step - loss: 0.6687 - acc: 0.7542 - val_loss: 0.7496 - val_acc: 0.7238\n",
      "Epoch 5/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.7812Epoch 00005: val_loss improved from 0.72222 to 0.70859, saving model to models/vgg_tune_dense.weights.best.hdf5\n",
      "72/71 [==============================] - 36s 500ms/step - loss: 0.6016 - acc: 0.7821 - val_loss: 0.7086 - val_acc: 0.7395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f5b4acf90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_weights_path = opath.join(MODEL_PATH, 'vgg_tune_dense.weights.best.hdf5')\n",
    "checkpointer = ModelCheckpoint(filepath=dense_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen)\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen)\n",
    "\n",
    "vgg_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//train_batches.batch_size,\n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//valid_batches.batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "vgg16 model finetune dense layer get accuary 0.751572325545%\n"
     ]
    }
   ],
   "source": [
    "vgg_model.load_weights(dense_weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (224, 224))\n",
    "\n",
    "print \"vgg16 model finetune dense layer get accuary {}%\".format(get_accuracy(vgg_model, test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg_model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "for layer in vgg_model.layers[25:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optmizer = SGD(0.0001, 0.9, 0.00001, True)\n",
    "vgg_model.compile(optimizer=optmizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2288 images belonging to 7 classes.\n",
      "Found 572 images belonging to 7 classes.\n",
      "Epoch 1/5\n",
      "70/71 [============================>.] - ETA: 2s - loss: 0.3426 - acc: 0.8746Epoch 00001: val_loss improved from inf to 0.62990, saving model to models/vgg_tune_conv.weights.best.hdf5\n",
      "72/71 [==============================] - 197s 3s/step - loss: 0.3381 - acc: 0.8754 - val_loss: 0.6299 - val_acc: 0.7850\n",
      "Epoch 2/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9201Epoch 00002: val_loss did not improve\n",
      "72/71 [==============================] - 23s 324ms/step - loss: 0.2154 - acc: 0.9210 - val_loss: 0.6326 - val_acc: 0.7972\n",
      "Epoch 3/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9496Epoch 00003: val_loss improved from 0.62990 to 0.61112, saving model to models/vgg_tune_conv.weights.best.hdf5\n",
      "72/71 [==============================] - 20s 271ms/step - loss: 0.1340 - acc: 0.9514 - val_loss: 0.6111 - val_acc: 0.7867\n",
      "Epoch 4/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9692Epoch 00004: val_loss did not improve\n",
      "72/71 [==============================] - 36s 496ms/step - loss: 0.1001 - acc: 0.9708 - val_loss: 0.6123 - val_acc: 0.8024\n",
      "Epoch 5/5\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9763Epoch 00005: val_loss did not improve\n",
      "72/71 [==============================] - 38s 522ms/step - loss: 0.0825 - acc: 0.9759 - val_loss: 0.6285 - val_acc: 0.8042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f5b315910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_weights_path = opath.join(MODEL_PATH, 'vgg_tune_conv.weights.best.hdf5')\n",
    "checkpointer = ModelCheckpoint(filepath=conv_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "train_batches = get_batches(TRAIN_PATH, train_datagen)\n",
    "valid_batches = get_batches(VALID_PATH, valid_datagen)\n",
    "\n",
    "vgg_model.fit_generator(train_batches, steps_per_epoch=train_batches.samples//train_batches.batch_size,\n",
    "                          validation_data=valid_batches, validation_steps=valid_batches.samples//valid_batches.batch_size,\n",
    "                          callbacks=[checkpointer],  epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 318 images belonging to 7 classes.\n",
      "vgg16 model finetune conv layer get accuary 0.779874212337%\n"
     ]
    }
   ],
   "source": [
    "vgg_model.load_weights(conv_weights_path)\n",
    "test_batches = get_batches(TEST_PATH, test_datagen, (224, 224))\n",
    "\n",
    "print \"vgg16 model finetune conv layer get accuary {}%\".format(get_accuracy(vgg_model, test_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
